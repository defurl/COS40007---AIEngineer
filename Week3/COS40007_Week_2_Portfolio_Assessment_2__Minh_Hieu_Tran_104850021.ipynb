{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "E6IrN_Fhm4Fw",
      "metadata": {
        "id": "E6IrN_Fhm4Fw"
      },
      "source": [
        "#### **Week 3: Portfolio Assessment**\n",
        "\n",
        "This notebook serves Porfolio Assessment in Week 3 of Unit `COS40007 - AI for Engineering`. All necessary inputs and data are configured automatically through downloading from google drive, so we don't need to change anything, just start and verify the results. My student ID ends with 1, so I will be using Shoulder - Left, Right, x, y, z\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Minh Hieu Tran** - 104850021"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e6b493",
      "metadata": {
        "id": "b8e6b493"
      },
      "source": [
        "### Step 1: Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa214b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa214b0",
        "outputId": "837038eb-22c4-4ccf-d95e-93df5d57ba1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boning data shape: (54180, 8)\n",
            "Slicing data shape: (17880, 8)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV files\n",
        "df_boning = pd.read_csv('https://raw.githubusercontent.com/defurl/COS40007---AIEngineer/refs/heads/main/Week3/Boning.csv')\n",
        "df_slicing = pd.read_csv('https://raw.githubusercontent.com/defurl/COS40007---AIEngineer/refs/heads/main/Week3/Slicing.csv')\n",
        "\n",
        "# Select only Right Shoulder and Left Shoulder\n",
        "boning_data = df_boning[['Frame', 'Right Shoulder x', 'Right Shoulder y', 'Right Shoulder z',\n",
        "                        'Left Shoulder x', 'Left Shoulder y', 'Left Shoulder z']].copy()\n",
        "slicing_data = df_slicing[['Frame', 'Right Shoulder x', 'Right Shoulder y', 'Right Shoulder z',\n",
        "                          'Left Shoulder x', 'Left Shoulder y', 'Left Shoulder z']].copy()\n",
        "\n",
        "# Add class labels\n",
        "boning_data['class'] = 0\n",
        "slicing_data['class'] = 1\n",
        "\n",
        "print(\"Boning data shape:\", boning_data.shape)\n",
        "print(\"Slicing data shape:\", slicing_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ade3037",
      "metadata": {
        "id": "4ade3037"
      },
      "source": [
        "### Step 2: Create Composite Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67f17905",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67f17905",
        "outputId": "7dc9dc22-82f9-47c2-e104-548b28193ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After adding composite features:\n",
            "Boning data shape: (54180, 20)\n",
            "Slicing data shape: (17880, 20)\n"
          ]
        }
      ],
      "source": [
        "def create_composite_features(data):\n",
        "  result = data.copy()\n",
        "\n",
        "  # For Right Shoulder\n",
        "  rs_x, rs_y, rs_z = data['Right Shoulder x'], data['Right Shoulder y'], data['Right Shoulder z']\n",
        "\n",
        "  # RMS calculations for Right Shoulder\n",
        "  result['Right_Shoulder_RMS_xy'] = np.sqrt((rs_x**2 + rs_y**2) / 2)\n",
        "  result['Right_Shoulder_RMS_yz'] = np.sqrt((rs_y**2 + rs_z**2) / 2)\n",
        "  result['Right_Shoulder_RMS_zx'] = np.sqrt((rs_z**2 + rs_x**2) / 2)\n",
        "  result['Right_Shoulder_RMS_xyz'] = np.sqrt((rs_x**2 + rs_y**2 + rs_z**2) / 3)\n",
        "\n",
        "  # Roll and Pitch for Right Shoulder\n",
        "  result['Right_Shoulder_Roll'] = 180 * np.arctan2(rs_y, np.sqrt(rs_x**2 + rs_z**2)) / np.pi\n",
        "  result['Right_Shoulder_Pitch'] = 180 * np.arctan2(rs_x, np.sqrt(rs_y**2 + rs_z**2)) / np.pi\n",
        "\n",
        "  # For Left Shoulder\n",
        "  rs_x, ls_y, ls_z = data['Left Shoulder x'], data['Left Shoulder y'], data['Left Shoulder z']\n",
        "\n",
        "  # RMS calculations for Left Shoulder\n",
        "  result['Left_Shoulder_RMS_xy'] = np.sqrt((rs_x**2 + ls_y**2) / 2)\n",
        "  result['Left_Shoulder_RMS_yz'] = np.sqrt((ls_y**2 + ls_z**2) / 2)\n",
        "  result['Left_Shoulder_RMS_zx'] = np.sqrt((ls_z**2 + rs_x**2) / 2)\n",
        "  result['Left_Shoulder_RMS_xyz'] = np.sqrt((rs_x**2 + ls_y**2 + ls_z**2) / 3)\n",
        "\n",
        "  # Roll and Pitch for Left Shoulder\n",
        "  result['Left_Shoulder_Roll'] = 180 * np.arctan2(ls_y, np.sqrt(rs_x**2 + ls_z**2)) / np.pi\n",
        "  result['Left_Shoulder_Pitch'] = 180 * np.arctan2(rs_x, np.sqrt(ls_y**2 + ls_z**2)) / np.pi\n",
        "\n",
        "  return result\n",
        "\n",
        "# Apply composite feature creation to both datasets\n",
        "boning_composite = create_composite_features(boning_data)\n",
        "slicing_composite = create_composite_features(slicing_data)\n",
        "\n",
        "print(\"After adding composite features:\")\n",
        "print(\"Boning data shape:\", boning_composite.shape)\n",
        "print(\"Slicing data shape:\", slicing_composite.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e2071b",
      "metadata": {
        "id": "f7e2071b"
      },
      "source": [
        "### Step 3: Data Pre-processing & Feature Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee0a002e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee0a002e",
        "outputId": "5348e0a3-5b23-41d2-8459-cc978f2dad1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boning data: 903 minutes\n",
            "Slicing data: 298 minutes\n"
          ]
        }
      ],
      "source": [
        "from scipy.signal import find_peaks\n",
        "import numpy as np\n",
        "\n",
        "def compute_statistical_features(group):\n",
        "  features = {}\n",
        "\n",
        "  # Get all columns except Frame, class, and minute\n",
        "  feature_cols = [col for col in group.columns if col not in ['Frame', 'class', 'minute']]\n",
        "\n",
        "  for col in feature_cols:\n",
        "    values = group[col].values\n",
        "\n",
        "    # Mean\n",
        "    features[f'{col}_mean'] = np.mean(values)\n",
        "    # Standard deviation\n",
        "    features[f'{col}_std'] = np.std(values)\n",
        "    # Min\n",
        "    features[f'{col}_min'] = np.min(values)\n",
        "    # Max\n",
        "    features[f'{col}_max'] = np.max(values)\n",
        "    # Area under curve (AUC) using trapezoidal rule\n",
        "    features[f'{col}_auc'] = np.trapz(np.abs(values))\n",
        "    # Number of peaks\n",
        "    peaks, _ = find_peaks(values, height=np.mean(values))\n",
        "    features[f'{col}_num_peaks'] = len(peaks)\n",
        "\n",
        "  return pd.Series(features)\n",
        "\n",
        "def group_by_minute(data):\n",
        "    data['minute'] = data['Frame'] // 60\n",
        "    return data\n",
        "\n",
        "# Group by minutes\n",
        "boning_grouped = group_by_minute(boning_composite)\n",
        "slicing_grouped = group_by_minute(slicing_composite)\n",
        "\n",
        "print(f\"Boning data: {len(boning_grouped['minute'].unique())} minutes\")\n",
        "print(f\"Slicing data: {len(slicing_grouped['minute'].unique())} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c9064504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9064504",
        "outputId": "2a628682-1b72-446d-e652-4332cd93c723"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3634883/2601076080.py:22: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  features[f'{col}_auc'] = np.trapz(np.abs(values))\n",
            "/tmp/ipykernel_3634883/144795264.py:2: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  boning_features = boning_grouped.groupby('minute').apply(compute_statistical_features).reset_index()\n",
            "/tmp/ipykernel_3634883/2601076080.py:22: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  features[f'{col}_auc'] = np.trapz(np.abs(values))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset shape: (1201, 109)\n",
            "Number of features (excluding class): 108\n",
            "\n",
            "Class distribution:\n",
            "class\n",
            "0    903\n",
            "1    298\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3634883/144795264.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  slicing_features = slicing_grouped.groupby('minute').apply(compute_statistical_features).reset_index()\n"
          ]
        }
      ],
      "source": [
        "# Compute statistical features for each minute\n",
        "boning_features = boning_grouped.groupby('minute').apply(compute_statistical_features).reset_index()\n",
        "slicing_features = slicing_grouped.groupby('minute').apply(compute_statistical_features).reset_index()\n",
        "\n",
        "# Add class labels\n",
        "boning_features['class'] = 0\n",
        "slicing_features['class'] = 1\n",
        "\n",
        "# Combine datasets\n",
        "final_dataset = pd.concat([boning_features, slicing_features], ignore_index=True)\n",
        "\n",
        "# Remove the minute column\n",
        "final_dataset = final_dataset.drop('minute', axis=1)\n",
        "\n",
        "print(\"Final dataset shape:\", final_dataset.shape)\n",
        "print(\"Number of features (excluding class):\", final_dataset.shape[1] - 1)\n",
        "print(\"\\nClass distribution:\")\n",
        "print(final_dataset['class'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114c40e2",
      "metadata": {
        "id": "114c40e2"
      },
      "source": [
        "### Step 4: Training - SVM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f8ad44e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6dd5fd90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd5fd90",
        "outputId": "24b54577-964d-4841-e496-e275da830cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Dataset for training: =====\n",
            "Features: (1201, 108)\n",
            "Labels: (1201,)\n",
            "\n",
            "Train set: (840, 108)\n",
            "Test set: (361, 108)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data preparation\n",
        "X = final_dataset.drop('class', axis=1)\n",
        "y = final_dataset['class']\n",
        "\n",
        "print(\"===== Dataset for training: =====\")\n",
        "print(f\"Features: {X.shape}\")\n",
        "print(f\"Labels: {y.shape}\")\n",
        "\n",
        "# Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27, stratify=y)\n",
        "print(f\"\\nTrain set: {X_train.shape}\\nTest set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db991de6",
      "metadata": {
        "id": "db991de6"
      },
      "source": [
        "#### 1 - 2: Basic SVM with train-test split (70/30) and 10-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "27d3197c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27d3197c",
        "outputId": "4713f395-936d-4bef-b25f-0750a9163c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Test Split Accuracy: 0.7507\n"
          ]
        }
      ],
      "source": [
        "# SVM\n",
        "svm_test = SVC(random_state=27)\n",
        "svm_test.fit(X_train, y_train)\n",
        "y_pred_test = svm_test.predict(X_test)\n",
        "accuracy_1_train_test = accuracy_score(y_test, y_pred_test)\n",
        "print(f\"Train-Test Split Accuracy: {accuracy_1_train_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9669073b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9669073b",
        "outputId": "1857e328-5533-451f-f077-b24e81be40e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-Fold CV Accuracy: 0.7519 ± 0.0033\n"
          ]
        }
      ],
      "source": [
        "# SVM 10-fold\n",
        "svm_basic_cv = SVC(random_state=27)\n",
        "cv_scores_basic = cross_val_score(svm_basic_cv, X, y, cv=10)\n",
        "accuracy_2_cv = cv_scores_basic.mean()\n",
        "std_2_cv = cv_scores_basic.std()\n",
        "print(f\"10-Fold CV Accuracy: {accuracy_2_cv:.4f} ± {std_2_cv:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9207f268",
      "metadata": {
        "id": "9207f268"
      },
      "source": [
        "#### 3: Hyperparameter tuning for BOTH train-test split AND 10-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f2760a0f",
      "metadata": {
        "id": "f2760a0f"
      },
      "outputs": [],
      "source": [
        "# Param Grid for Grid Search\n",
        "param_grid = {\n",
        "  'C': [0.1, 1, 10, 100],\n",
        "  'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "  'kernel': ['rbf', 'linear']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d6b64659",
      "metadata": {
        "id": "d6b64659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "# Find best hyperparameters using GridSearch\n",
        "grid_search = GridSearchCV(SVC(random_state=27), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8f2c8124",
      "metadata": {
        "id": "8f2c8124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Test Split with Hyperparameter Tuning:\n",
            "Train-Test Split Accuracy: 0.7867\n"
          ]
        }
      ],
      "source": [
        "# Apply hyperparameter tuning\n",
        "print(\"Train-Test Split with Hyperparameter Tuning:\")\n",
        "y_pred_3a = grid_search.predict(X_test)\n",
        "accuracy_3a_train_test = accuracy_score(y_test, y_pred_3a)\n",
        "print(f\"Train-Test Split Accuracy: {accuracy_3a_train_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dccdb942",
      "metadata": {
        "id": "dccdb942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-Fold CV with Hyperparameter Tuning:\n",
            "10-Fold CV Accuracy: 0.7943 ± 0.0480\n"
          ]
        }
      ],
      "source": [
        "# Apply hyperparameter tuning to 10-fold Cross-Validation\n",
        "print(\"10-Fold CV with Hyperparameter Tuning:\")\n",
        "# Use the best estimator for cross-validation\n",
        "best_svm = grid_search.best_estimator_\n",
        "cv_scores_3b = cross_val_score(best_svm, X, y, cv=10)\n",
        "accuracy_3b_cv = cv_scores_3b.mean()\n",
        "std_3b_cv = cv_scores_3b.std()\n",
        "print(f\"10-Fold CV Accuracy: {accuracy_3b_cv:.4f} ± {std_3b_cv:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285977e9",
      "metadata": {
        "id": "285977e9"
      },
      "source": [
        "#### Step 4: Hyperparameter tuning + 10 best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f9aba97f",
      "metadata": {
        "id": "f9aba97f"
      },
      "outputs": [],
      "source": [
        "# Select 10 best features\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b07a0d97",
      "metadata": {
        "id": "b07a0d97"
      },
      "outputs": [],
      "source": [
        "# Split the selected features for train-test\n",
        "X_train_selected, X_test_selected, _, _ = train_test_split(X_selected, y, test_size=0.3, random_state=27, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "80113ed1",
      "metadata": {
        "id": "80113ed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Test Split + Tuning + 10 Best Features:\n",
            "Train-Test Split Accuracy: 0.7784\n"
          ]
        }
      ],
      "source": [
        "# 4a) Train-Test Split with Hyperparameter Tuning + 10 Best Features\n",
        "print(\"Train-Test Split + Tuning + 10 Best Features:\")\n",
        "grid_search_4a = GridSearchCV(SVC(random_state=27), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_4a.fit(X_train_selected, y_train)\n",
        "y_pred_4a = grid_search_4a.predict(X_test_selected)\n",
        "accuracy_4a_train_test = accuracy_score(y_test, y_pred_4a)\n",
        "print(f\"Train-Test Split Accuracy: {accuracy_4a_train_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "89660ada",
      "metadata": {
        "id": "89660ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-Fold CV + Tuning + 10 Best Features:\n",
            "10-Fold CV Accuracy: 0.8010 ± 0.0369\n"
          ]
        }
      ],
      "source": [
        "# 4b) 10-Fold CV with Hyperparameter Tuning + 10 Best Features\n",
        "print(\"10-Fold CV + Tuning + 10 Best Features:\")\n",
        "best_svm_4b = grid_search_4a.best_estimator_\n",
        "cv_scores_4b = cross_val_score(best_svm_4b, X_selected, y, cv=10)\n",
        "accuracy_4b_cv = cv_scores_4b.mean()\n",
        "std_4b_cv = cv_scores_4b.std()\n",
        "print(f\"10-Fold CV Accuracy: {accuracy_4b_cv:.4f} ± {std_4b_cv:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32a75d6",
      "metadata": {
        "id": "a32a75d6"
      },
      "source": [
        "#### Step 5: Hyperparameter tuning + PCA (n_components=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "0d31ecd6",
      "metadata": {
        "id": "0d31ecd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying PCA to extract 10 principal components...\n",
            "PCA explained variance ratio: 0.7937\n"
          ]
        }
      ],
      "source": [
        "# Apply PCA\n",
        "print(\"Applying PCA to extract 10 principal components...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=10, random_state=27)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Split the PCA features for train-test\n",
        "X_train_pca, X_test_pca, _, _ = train_test_split(X_pca, y, test_size=0.3, random_state=27, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "13978f9f",
      "metadata": {
        "id": "13978f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train-Test Split + Tuning + 10 PCA Components:\n",
            "Train-Test Split Accuracy: 0.8033\n"
          ]
        }
      ],
      "source": [
        "# Train-Test Split with Hyperparameter Tuning + 10 PCA Components\n",
        "print(\"Train-Test Split + Tuning + 10 PCA Components:\")\n",
        "grid_search_5a = GridSearchCV(SVC(random_state=27), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_5a.fit(X_train_pca, y_train)\n",
        "y_pred_5a = grid_search_5a.predict(X_test_pca)\n",
        "accuracy_5a_train_test = accuracy_score(y_test, y_pred_5a)\n",
        "print(f\"Train-Test Split Accuracy: {accuracy_5a_train_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "021e8ead",
      "metadata": {
        "id": "021e8ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10-Fold CV + Tuning + 10 PCA Components:\n",
            "10-Fold CV Accuracy: 0.8127 ± 0.0525\n"
          ]
        }
      ],
      "source": [
        "# 10-Fold CV with Hyperparameter Tuning + 10 PCA Components\n",
        "print(\"10-Fold CV + Tuning + 10 PCA Components:\")\n",
        "best_svm_5b = grid_search_5a.best_estimator_\n",
        "cv_scores_5b = cross_val_score(best_svm_5b, X_pca, y, cv=10)\n",
        "accuracy_5b_cv = cv_scores_5b.mean()\n",
        "std_5b_cv = cv_scores_5b.std()\n",
        "print(f\"10-Fold CV Accuracy: {accuracy_5b_cv:.4f} ± {std_5b_cv:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "142b31eb",
      "metadata": {
        "id": "142b31eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMPLETE SVM SUMMARY TABLE\n",
            "                               Step                   Method        Accuracy\n",
            "                  Step 1: Basic SVM Train-Test Split (70/30)          0.7507\n",
            "                  Step 2: Basic SVM 10-Fold Cross-Validation 0.7519 ± 0.0033\n",
            "     Step 3a: Hyperparameter Tuning Train-Test Split (70/30)          0.7867\n",
            "     Step 3b: Hyperparameter Tuning 10-Fold Cross-Validation 0.7943 ± 0.0480\n",
            " Step 4a: Tuning + 10 Best Features Train-Test Split (70/30)          0.7784\n",
            " Step 4b: Tuning + 10 Best Features 10-Fold Cross-Validation 0.8010 ± 0.0369\n",
            "Step 5a: Tuning + 10 PCA Components Train-Test Split (70/30)          0.8033\n",
            "Step 5b: Tuning + 10 PCA Components 10-Fold Cross-Validation 0.8127 ± 0.0525\n",
            "\n",
            "BEST RESULTS:\n",
            "   Best Train-Test Split: Step 5a (+ PCA) = 0.8033\n",
            "   Best Cross-Validation: Step 5b (+ PCA) = 0.8127\n"
          ]
        }
      ],
      "source": [
        "# Create Complete SVM Summary Table\n",
        "print(\"COMPLETE SVM SUMMARY TABLE\")\n",
        "\n",
        "svm_results = pd.DataFrame({\n",
        "  'Step': [\n",
        "    'Step 1: Basic SVM',\n",
        "    'Step 2: Basic SVM',\n",
        "    'Step 3a: Hyperparameter Tuning',\n",
        "    'Step 3b: Hyperparameter Tuning',\n",
        "    'Step 4a: Tuning + 10 Best Features',\n",
        "    'Step 4b: Tuning + 10 Best Features',\n",
        "    'Step 5a: Tuning + 10 PCA Components',\n",
        "    'Step 5b: Tuning + 10 PCA Components'\n",
        "  ],\n",
        "  'Method': [\n",
        "    'Train-Test Split (70/30)',\n",
        "    '10-Fold Cross-Validation',\n",
        "    'Train-Test Split (70/30)',\n",
        "    '10-Fold Cross-Validation',\n",
        "    'Train-Test Split (70/30)',\n",
        "    '10-Fold Cross-Validation',\n",
        "    'Train-Test Split (70/30)',\n",
        "    '10-Fold Cross-Validation'\n",
        "  ],\n",
        "  'Accuracy': [\n",
        "    f\"{accuracy_1_train_test:.4f}\",\n",
        "    f\"{accuracy_2_cv:.4f} ± {std_2_cv:.4f}\",\n",
        "    f\"{accuracy_3a_train_test:.4f}\",\n",
        "    f\"{accuracy_3b_cv:.4f} ± {std_3b_cv:.4f}\",\n",
        "    f\"{accuracy_4a_train_test:.4f}\",\n",
        "    f\"{accuracy_4b_cv:.4f} ± {std_4b_cv:.4f}\",\n",
        "    f\"{accuracy_5a_train_test:.4f}\",\n",
        "    f\"{accuracy_5b_cv:.4f} ± {std_5b_cv:.4f}\"\n",
        "  ]\n",
        "})\n",
        "\n",
        "print(svm_results.to_string(index=False))\n",
        "\n",
        "# Find best performing methods\n",
        "train_test_scores = [\n",
        "  (\"Step 1 (Basic)\", accuracy_1_train_test),\n",
        "  (\"Step 3a (+ Tuning)\", accuracy_3a_train_test),\n",
        "  (\"Step 4a (+ Features)\", accuracy_4a_train_test),\n",
        "  (\"Step 5a (+ PCA)\", accuracy_5a_train_test)\n",
        "]\n",
        "\n",
        "cv_scores = [\n",
        "  (\"Step 2 (Basic)\", accuracy_2_cv),\n",
        "  (\"Step 3b (+ Tuning)\", accuracy_3b_cv),\n",
        "  (\"Step 4b (+ Features)\", accuracy_4b_cv),\n",
        "  (\"Step 5b (+ PCA)\", accuracy_5b_cv)\n",
        "]\n",
        "\n",
        "best_train_test = max(train_test_scores, key=lambda x: x[1])\n",
        "best_cv = max(cv_scores, key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\nBEST RESULTS:\")\n",
        "print(f\"   Best Train-Test Split: {best_train_test[0]} = {best_train_test[1]:.4f}\")\n",
        "print(f\"   Best Cross-Validation: {best_cv[0]} = {best_cv[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9418480",
      "metadata": {
        "id": "f9418480"
      },
      "source": [
        "### Step 5: Training - Other Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "121741a1",
      "metadata": {
        "id": "121741a1"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define models with preprocessing pipelines\n",
        "models = {\n",
        "  \"SGD\": Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SGDClassifier(loss=\"log_loss\", alpha=1e-4, max_iter=2000, random_state=42))\n",
        "  ]),\n",
        "  \"Random Forest\": Pipeline([\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "      n_estimators=100, max_depth=None, n_jobs=-1, random_state=42\n",
        "    ))\n",
        "  ]),\n",
        "  \"MLP\": Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", MLPClassifier(\n",
        "      hidden_layer_sizes=(100, 50),\n",
        "      activation=\"relu\",\n",
        "      solver=\"adam\",\n",
        "      max_iter=500,\n",
        "      random_state=42\n",
        "    ))\n",
        "  ]),\n",
        "  \"SVM (Best)\": Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", grid_search.best_estimator_)\n",
        "  ])\n",
        "}\n",
        "\n",
        "# Set up cross-validation\n",
        "cv = KFold(n_splits=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "31b3f462",
      "metadata": {
        "id": "31b3f462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training SGD:\n",
            "  Train-test accuracy: 0.7895\n",
            "  10-fold CV accuracy: 0.7877 ± 0.0299\n",
            "\n",
            "Training Random Forest:\n",
            "  Train-test accuracy: 0.8033\n",
            "  10-fold CV accuracy: 0.8126 ± 0.0253\n",
            "\n",
            "Training MLP:\n",
            "  Train-test accuracy: 0.7867\n",
            "  10-fold CV accuracy: 0.8185 ± 0.0295\n",
            "\n",
            "Training SVM (Best):\n",
            "  Train-test accuracy: 0.7756\n",
            "  10-fold CV accuracy: 0.8201 ± 0.0281\n"
          ]
        }
      ],
      "source": [
        "# Train models and collect results\n",
        "model_results = []\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "  print(f\"\\nTraining {name}:\")\n",
        "\n",
        "  # Train-test split accuracy\n",
        "  pipeline.fit(X_train, y_train)\n",
        "  y_pred = pipeline.predict(X_test)\n",
        "  train_test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "  # 10-fold cross-validation accuracy\n",
        "  cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
        "  cv_mean = cv_scores.mean()\n",
        "  cv_std = cv_scores.std()\n",
        "\n",
        "  model_results.append({\n",
        "    \"Model\": name,\n",
        "    \"Train-Test Split\": f\"{train_test_acc:.4f}\",\n",
        "    \"10-Fold Cross-Validation\": f\"{cv_mean:.4f} ± {cv_std:.4f}\"\n",
        "  })\n",
        "\n",
        "  print(f\"  Train-test accuracy: {train_test_acc:.4f}\")\n",
        "  print(f\"  10-fold CV accuracy: {cv_mean:.4f} ± {cv_std:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "HCOW9Y4Xq0da",
      "metadata": {
        "id": "HCOW9Y4Xq0da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MODEL COMPARISON TABLE\n",
            "        Model Train-Test Split 10-Fold Cross-Validation\n",
            "          SGD           0.7895          0.7877 ± 0.0299\n",
            "Random Forest           0.8033          0.8126 ± 0.0253\n",
            "          MLP           0.7867          0.8185 ± 0.0295\n",
            "   SVM (Best)           0.7756          0.8201 ± 0.0281\n"
          ]
        }
      ],
      "source": [
        "# Create results table\n",
        "results_df = pd.DataFrame(model_results)\n",
        "print(\"\\nMODEL COMPARISON TABLE\")\n",
        "print(results_df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cos40007-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
